# ðŸ§  MNIST Handwritten Digit Classification  
This project builds a **Deep Learning model** to classify handwritten digits (0-9) using the **MNIST dataset**.  
It includes **Batch Normalization, Dropout, and Hyperparameter Tuning** to improve accuracy.  

---

## ðŸ“Œ Project Overview  
MNIST is a dataset of **28x28 grayscale images** of handwritten digits (0-9).  
The goal is to train a neural network to accurately classify these digits.  

ðŸ”¹ **Dataset:** [MNIST Handwritten Digits](http://yann.lecun.com/exdb/mnist/)  
ðŸ”¹ **Libraries Used:** `TensorFlow`, `Keras`, `Matplotlib`, `NumPy`  
ðŸ”¹ **Techniques Implemented:**  
   âœ… Fully Connected Neural Network (MLP)  
   âœ… Batch Normalization & Dropout (Prevent Overfitting)  
   âœ… Hyperparameter Tuning (Keras Tuner)  
   âœ… Model Evaluation (Accuracy & Loss Plots)  

---

## ðŸš€ Features  
âœ” **98.03% Test Accuracy** on MNIST dataset  
âœ” **Batch Normalization & Dropout** for better generalization  
âœ” **Hyperparameter Optimization** using Keras Tuner  
âœ” **Visualized Accuracy & Loss Graphs**  

---

## ðŸ“‚ Project Structure  
```bash
mnist-digit-classification/
â”‚â”€â”€ notebooks/                  # Jupyter notebooks (if applicable)
â”‚â”€â”€ results/                     # Accuracy reports & loss curves
â”‚â”€â”€ mnist_classification.py      # Main Python script
â”‚â”€â”€ hyperparameter_tuning.py     # Keras Tuner script for optimization
â”‚â”€â”€ requirements.txt             # Dependencies
â”‚â”€â”€ README.md                    # Project documentation
```

## âš¡ Model Architecture
The model consists of:

âœ” Flatten Layer â€“ Converts 2D images into 1D vectors

âœ” Dense Layers â€“ Fully connected layers for learning patterns

âœ” Batch Normalization â€“ Normalizes activations to stabilize training
âœ” Dropout â€“ Reduces overfitting
âœ” Softmax Layer â€“ Outputs probabilities for 10 classes (digits 0-9)
