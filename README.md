# ðŸ§  MNIST Handwritten Digit Classification  
This project builds a **Deep Learning model** to classify handwritten digits (0-9) using the **MNIST dataset**.  
It includes **Batch Normalization, Dropout, and Hyperparameter Tuning** to improve accuracy.  

---

## ðŸ“Œ Project Overview  
MNIST is a dataset of **28x28 grayscale images** of handwritten digits (0-9).  
The goal is to train a neural network to accurately classify these digits.  

ðŸ”¹ **Dataset:** [MNIST Handwritten Digits](http://yann.lecun.com/exdb/mnist/)  
ðŸ”¹ **Libraries Used:** `TensorFlow`, `Keras`, `Matplotlib`, `NumPy`  
ðŸ”¹ **Techniques Implemented:**  
   âœ… Fully Connected Neural Network (MLP)  
   âœ… Batch Normalization & Dropout (Prevent Overfitting)  
   âœ… Hyperparameter Tuning (Keras Tuner)  
   âœ… Model Evaluation (Accuracy & Loss Plots)  

---

## ðŸš€ Features  
âœ” **98.03% Test Accuracy** on MNIST dataset  
âœ” **Batch Normalization & Dropout** for better generalization  
âœ” **Hyperparameter Optimization** using Keras Tuner  
âœ” **Visualized Accuracy & Loss Graphs**  

---

## ðŸ“‚ Project Structure  
```bash
mnist-digit-classification/
â”‚â”€â”€ notebooks/                  # Jupyter notebooks (if applicable)
â”‚â”€â”€ results/                     # Accuracy reports & loss curves
â”‚â”€â”€ mnist_classification.py      # Main Python script
â”‚â”€â”€ hyperparameter_tuning.py     # Keras Tuner script for optimization
â”‚â”€â”€ requirements.txt             # Dependencies
â”‚â”€â”€ README.md                    # Project documentation
```

## âš¡ Model Architecture
The model consists of:

âœ” Flatten Layer â€“ Converts 2D images into 1D vectors

âœ” Dense Layers â€“ Fully connected layers for learning patterns

âœ” Batch Normalization â€“ Normalizes activations to stabilize training

âœ” Dropout â€“ Reduces overfitting

âœ” Softmax Layer â€“ Outputs probabilities for 10 classes (digits 0-9)

## ðŸ“Š Accuracy & Loss Graphs
### Training vs. Validation Accuracy
![image](https://github.com/user-attachments/assets/58de177e-5fbe-48b6-a883-6e6b6f47ef28)

### Training vs. Validation Loss
![image](https://github.com/user-attachments/assets/588b8cc4-e90e-43d3-886a-67817d31b4b1)


ðŸŽ¯ Results
| Model Version |	Test Accuracy |	Loss |
| -------------| ------------ | --------- |
| Baseline Model |	97.74%	| 0.0764 |
| Tuned Model	| 98.03%	| 0.0636 |



